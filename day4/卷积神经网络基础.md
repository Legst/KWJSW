# transformer 知识总结

Transformer 是一种完全基于注意力机制的模型架构，它已经成为自然语言处理和计算机视觉等多个 AI 领域的核心方法。

---


## 目录
1. 我的的理解
2. 什么是Self-attention
3. 什么是Q、K、V
4. 什么是Multi-head attention
5. 什么是位置编码，解决什么问题
6. 如何理解transformer的并行运算
7. self-attention pytorch 代码
1. 导入必要的库
2. 定义自注意力模块类 `selfAttention`
3. 定义将张量转换为多头形式的函数 `trans_to_multiple_heads`
4. 定义前向传播函数 `forward`
5. 测试部分

## 1. 我的的理解
最开始运用在NLP中的，它就相当于一个Attention结构，它相当于将一张图片平均分割成很多窗口，每个窗口相当于NLP里面的每个词，如果在目标检测中就是每个序列，然后计算每个序列上下文关系，然后将所有的关系融入在一起，这样就能够直接获取全局信息了，而不像CNN那样需要逐步递归才能获得全局信息，也不像RNN那样速度很慢，是因为它可以并行计算。

## 2. 什么是Self-attention

self-attention就是自注意机制，也就是说当前时刻的输入不止关注当前时刻的信息，还会关注其它时刻的或者说关注所有的时刻信息，计算出其相关性，得到注意力权重矩阵。

## 3. 什么是Q、K、V

首先Attention的任务是获取局部关注的信息。Attention的引入让我们知道输入数据中，哪些地方更值得关注。
Q、K、V都源于输入特征本身，是根据输入特征产生的向量。V可以看做表示单个输入特征的向量，我们直接将V输入到网络中进行训练是没有引入Attention的网络。如果要引入Attention网络就需要通过V乘以一组权重，这个权重由Q和K计算得来，就可以做到关注局部输入特征。

V：输入特征的向量 Q和K：计算Attention权重的特征向量。
Attention机制中的Q,K,V：我们对当前的Query和所有的Key计算相似度，将这个相似度值通过Softmax层进行得到一组权重，根据这组权重与对应Value的乘积求和得到Attention下的Value值。
## 4. 什么是Multi-head attention

multi-head attention是多个自注意机制模块，通过对self-attention赋予不一样的权重，来得到不一样的结果，并把所有的attention结果拼接起来，通过一个全连接层得到最终结果，从而有助于捕捉到更丰富特征。

## 5. 什么是位置编码，解决什么问题
由于同一张映射图，目标在不同的位置，Transormer中attention机制并没有包含其位置信息，是不能够进行有效区分的，比如说最简单的我喜欢你和你喜欢我是不一样的结果，所以我们通过加入了位置编码，这样就能够区分到底是谁喜欢谁这一问题，也就是说模型可以考虑前后位置的关系。

## 6. 如何理解transformer的并行运算
最核心的在multi-head attention ，多组KQV进行self-attention运算，它们是可以同时运算的，由于使用同步运算，所以对于硬件要求比较高。

## 7. self-attention pytorch 代码
这段代码实现了一个基于 PyTorch 的自注意力（Self-Attention）模块。自注意力机制在自然语言处理和其他领域中被广泛应用，用于计算输入序列中各个元素之间的相关性。下面对代码进行详细分析：

### 1. 导入必要的库

```python
import torch
import numpy as np
import torch.nn as nn
import math
import torch.nn.functional as F
```

导入了 PyTorch 库 torch，用于处理张量和神经网络；numpy 用于数值计算；nn 是 PyTorch 的神经网络模块；math 用于数学运算；F 是 torch.nn.functional 模块，包含了许多常用的神经网络函数。

### 2. 定义自注意力模块类 selfAttention

```python
class selfAttention(nn.Module) :
    def __init__(self, num_attention_heads, input_size, hidden_size):
        super(selfAttention, self).__init__()
        if hidden_size % num_attention_heads!= 0 :
            raise ValueError(
                "the hidden size %d is not a multiple of the number of attention heads"
                "%d" % (hidden_size, num_attention_heads)
            )

        self.num_attention_heads = num_attention_heads
        self.attention_head_size = int(hidden_size / num_attention_heads)
        self.all_head_size = hidden_size

        self.key_layer = nn.Linear(input_size, hidden_size)
        self.query_layer = nn.Linear(input_size, hidden_size)
        self.value_layer = nn.Linear(input_size, hidden_size)
```

初始化函数 __init__：
参数：
num_attention_heads：注意力头的数量。
input_size：输入特征的维度。
hidden_size：隐藏层的维度，也是多头注意力输出的总维度。
检查 hidden_size 是否是 num_attention_heads 的倍数：如果不是，抛出 ValueError 异常。
计算每个注意力头的维度 attention_head_size：hidden_size 除以 num_attention_heads。
定义线性变换层：
key_layer：将输入特征映射到 hidden_size 维度，用于生成键（Key）。
query_layer：将输入特征映射到 hidden_size 维度，用于生成查询（Query）。
value_layer：将输入特征映射到 hidden_size 维度，用于生成值（Value）。

### 3. 定义将张量转换为多头形式的函数 trans_to_multiple_heads

```python
def trans_to_multiple_heads(self, x):
    new_size = x.size()[ : -1] + (self.num_attention_heads, self.attention_head_size)
    x = x.view(new_size)
    return x.permute(0, 2, 1, 3)
```
参数：
x：输入张量。
步骤：
调整张量形状：将输入张量 x 的形状调整为 (batch_size, num_attention_heads, sequence_length, attention_head_size)。
维度变换：使用 permute 函数将维度顺序调整为 (batch_size, num_attention_heads, sequence_length, attention_head_size)，以便后续进行矩阵乘法运算。

### 4. 定义前向传播函数 forward

```python
def forward(self, x):
    key = self.key_layer(x)
    query = self.query_layer(x)
    value = self.value_layer(x)

    key_heads = self.trans_to_multiple_heads(key)
    query_heads = self.trans_to_multiple_heads(query)
    value_heads = self.trans_to_multiple_heads(value)

    attention_scores = torch.matmul(query_heads, key_heads.permute(0, 1, 3, 2))
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)

    attention_probs = F.softmax(attention_scores, dim = -1)

    context = torch.matmul(attention_probs, value_heads)
    context = context.permute(0, 2, 1, 3).contiguous()
    new_size = context.size()[ : -2] + (self.all_head_size, )
    context = context.view(*new_size)
    return context
```

步骤：
生成键、查询和值：
使用 key_layer、query_layer 和 value_layer 对输入 x 进行线性变换，得到键 key、查询 query 和值 value。
将键、查询和值转换为多头形式：
使用 trans_to_multiple_heads 函数将 key、query 和 value 转换为多头形式，得到 key_heads、query_heads 和 value_heads。
计算注意力分数：
使用 torch.matmul 函数计算查询和键之间的矩阵乘法，得到注意力分数 attention_scores。
将注意力分数除以 math.sqrt(self.attention_head_size) 进行缩放，以稳定训练过程。
计算注意力概率：
使用 F.softmax 函数对注意力分数进行 softmax 操作，得到注意力概率 attention_probs。
计算上下文向量：
使用 torch.matmul 函数将注意力概率和值进行矩阵乘法，得到上下文向量 context。
对上下文向量进行维度变换和形状调整，使其恢复到原始的形状，最后返回上下文向量。

### 5. 测试部分
```python
features = torch.rand((32, 20, 10))
attention = selfAttention(2, 10, 20)
result = attention.forward(features)
print(result.shape) # torch.Size([32, 20, 20])
```
生成随机输入特征：
features 是一个形状为 (32, 20, 10) 的随机张量，其中 32 是批量大小，20 是序列长度，10 是输入特征的维度。
创建自注意力模块实例：
attention 是一个 selfAttention 类的实例，设置注意力头的数量为 2，输入特征维度为 10，隐藏层维度为 20。
进行前向传播并打印结果形状：
使用 attention.forward(features) 进行前向传播，得到输出结果 result。
打印结果的形状，输出为 torch.Size([32, 20, 20])，表示输出的批量大小为 32，序列长度为 20，特征维度为 20。

                        


